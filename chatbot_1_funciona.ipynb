{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "033fbd7d97134b4b9569497d927d6dd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dbec58563d0c485bacabb65ce9ef8b1c",
              "IPY_MODEL_d9b54f7876d7489ca13037c83f1502b4",
              "IPY_MODEL_9b06936cf96941f1a2b9b01cc5694a02"
            ],
            "layout": "IPY_MODEL_be2107cce4b747a6b87d1f20a64060fb"
          }
        },
        "dbec58563d0c485bacabb65ce9ef8b1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10c9f46b9dbc4279b6be61af2fdcb5b6",
            "placeholder": "​",
            "style": "IPY_MODEL_7bb1a68fb0b849ad83622ff19ebc2df5",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "d9b54f7876d7489ca13037c83f1502b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06782c011b8c41cebdaf17a541a00be6",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_971a586db4b2442080b85e4d819ac33a",
            "value": 8
          }
        },
        "9b06936cf96941f1a2b9b01cc5694a02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92485fea58e44a298dfe920e72d01e60",
            "placeholder": "​",
            "style": "IPY_MODEL_9597a5cd06274d148f5edc8d1adb26de",
            "value": " 8/8 [54:20&lt;00:00, 456.54s/it]"
          }
        },
        "be2107cce4b747a6b87d1f20a64060fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10c9f46b9dbc4279b6be61af2fdcb5b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bb1a68fb0b849ad83622ff19ebc2df5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06782c011b8c41cebdaf17a541a00be6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "971a586db4b2442080b85e4d819ac33a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "92485fea58e44a298dfe920e72d01e60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9597a5cd06274d148f5edc8d1adb26de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# INSTALACION DE LIBRERIAS"
      ],
      "metadata": {
        "id": "DMpap-8YpMN8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtPxHUzJzp4I"
      },
      "outputs": [],
      "source": [
        "!pip install bitsandbytes accelerate xformers einops langchain faiss-cpu transformers pypdf sentence-transformers -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORTS E INICIALIZACION"
      ],
      "metadata": {
        "id": "vuRy6I3npTKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig\n",
        "import torch\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
        "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
        "from langchain_core.vectorstores import VectorStoreRetriever\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device:\", device)\n",
        "if device == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0ZQGa3i0EN1",
        "outputId": "1c7d1f69-6497-4707-daad-638a24ec14df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "orig_model_path = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "model_path = \"filipealmeida/Mistral-7B-Instruct-v0.1-sharded\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "                                load_in_4bit=True,\n",
        "                                bnb_4bit_use_double_quant=True,\n",
        "                                bnb_4bit_quant_type=\"nf4\",\n",
        "                                bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "                               )\n",
        "if device == 'cuda':\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, quantization_config=bnb_config, device_map=\"auto\") #esto parece que solo anda con gpu t4\n",
        "else\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, device_map=\"auto\",offload_folder=\"/content/extra\") #cpu (se queda sin ram colab y se rompe)\n",
        "tokenizer = AutoTokenizer.from_pretrained(orig_model_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "033fbd7d97134b4b9569497d927d6dd3",
            "dbec58563d0c485bacabb65ce9ef8b1c",
            "d9b54f7876d7489ca13037c83f1502b4",
            "9b06936cf96941f1a2b9b01cc5694a02",
            "be2107cce4b747a6b87d1f20a64060fb",
            "10c9f46b9dbc4279b6be61af2fdcb5b6",
            "7bb1a68fb0b849ad83622ff19ebc2df5",
            "06782c011b8c41cebdaf17a541a00be6",
            "971a586db4b2442080b85e4d819ac33a",
            "92485fea58e44a298dfe920e72d01e60",
            "9597a5cd06274d148f5edc8d1adb26de"
          ]
        },
        "id": "vI9OkYc00J2D",
        "outputId": "53e055f4-054c-4226-86cf-6fed4914cc51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "033fbd7d97134b4b9569497d927d6dd3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu and disk.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_generation_pipeline = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=True,\n",
        "    max_new_tokens=100,\n",
        ")\n",
        "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
      ],
      "metadata": {
        "id": "0JnQjRrg0PrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PRUEBAS\n",
        "se pueden omitir, las hice para ver como funcionan algunas cosas"
      ],
      "metadata": {
        "id": "86bTv2x3o3xy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"que es mistral? escribe una respuesta corta.\"\n",
        "mistral_llm.invoke(text)\n",
        "\n",
        "#> Mistral is a type of cold front that forms over the Mediterranean\n",
        "#> Sea and moves eastward across southern Europe, bringing strong winds\n",
        "#> and sometimes severe weather conditions such as heavy rainfall, hail,\n",
        "#> and even tornadoes."
      ],
      "metadata": {
        "id": "zH7_dbZo0Ug-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate.from_template(\n",
        "    \"dime un chiste {adjective} sobre {content}.\"\n",
        ")\n",
        "prompt.format(adjective=\"divertido\", content=\"pollos\")\n",
        "\n",
        "llm_chain = prompt | mistral_llm\n",
        "llm_chain.invoke({\"adjective\": \"divertido\", \"content\": \"pollos\"})"
      ],
      "metadata": {
        "id": "4-ufc6rv0XV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = prompt | mistral_llm\n",
        "llm_chain.invoke({\"adjective\": \"divertido\", \"content\": \"pollos\"},\n",
        "                 config={'callbacks': [ConsoleCallbackHandler()]})"
      ],
      "metadata": {
        "id": "ppVRtWl40deI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AGREGO PDFS Y GUARDO EN DB VECTORIAL"
      ],
      "metadata": {
        "id": "6V3mrNGWpgZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#connect to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xqlSUmXOEEyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# carga de PDFs del directorio\n",
        "loader = PyPDFDirectoryLoader(\"/content/drive/MyDrive/milei/\")\n",
        "data = loader.load()\n",
        "\n",
        "# Text chunking\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=20)\n",
        "text_chunks = text_splitter.split_documents(data)\n",
        "\n",
        "# preparacion de textos, metadata e ids\n",
        "documents = []\n",
        "metadata = []\n",
        "ids = []\n",
        "\n",
        "#print(text_chunks[2].chunk)\n",
        "for i, chunk in enumerate(text_chunks):\n",
        "  documents.append(chunk.page_content)\n",
        "  metadata.append(data[i].metadata)\n",
        "  ids.append(str(i))\n",
        "\n",
        "# embeddings\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-l6-v2\",\n",
        "    model_kwargs={\"device\": \"cuda\"},\n",
        ")\n",
        "\n",
        "vector_db = FAISS.from_texts(texts=documents, embedding=embeddings, metadatas=metadata, ids=ids)\n",
        "retriever = VectorStoreRetriever(vectorstore=vector_db, k=3)"
      ],
      "metadata": {
        "id": "lgNtWK0s0ubU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"eres una IA asistente muy servicial. usa las siguientes partes de contexto para responder al final.\n",
        "              {context}\n",
        "              historial de chat: {history}\n",
        "              pregunta: {question}\n",
        "              respuesta:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "        template=template, input_variables=[\"history\", \"context\", \"question\"],\n",
        "        hide_template=True,  # Oculta el template del log\n",
        "    )\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "        llm=mistral_llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever)\n",
        "#        chain_type_kwargs={\n",
        "#            \"verbose\": True,\n",
        "#            \"prompt\": prompt,\n",
        "#            \"memory\": ConversationBufferMemory(\n",
        "#                memory_key=\"history\",\n",
        "#                input_key=\"question\"),\n",
        "#        }\n",
        "#    )\n",
        "# deje comentado la agregacion del template a la definicion de qa"
      ],
      "metadata": {
        "id": "Z2OK6Ibx0xQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa.invoke(\"nombrame un articulo de la constitucion nacional\")"
      ],
      "metadata": {
        "id": "gWhhX5mV0z1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "while True:\n",
        "  user_input = input(f\"Input Prompt: \")\n",
        "  if user_input == 'exit':\n",
        "    print('Exiting')\n",
        "    sys.exit()\n",
        "  if user_input == '':\n",
        "    continue\n",
        "  result = qa.invoke({'query': user_input})\n",
        "  print(f\"Answer: {result['result']}\")\n",
        "\n",
        "#me responde en ingles por mas que la pregunta y la documentacion estan en español"
      ],
      "metadata": {
        "id": "TwJFoTk6He40"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}